{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acea8d5-bbcd-4050-88e7-4ddd048d656d",
   "metadata": {},
   "source": [
    "# **Pipeline for Deep Learning: Reconstruction and Map Construction from MS39 CSV File**\n",
    "\n",
    "This file encompasses two major steps to prepare data for deep learning applications in **corneal ophthalmology**.\n",
    "\n",
    "## **Step 1: Matrix Reconstruction (Polar to Cartesian)**\n",
    "- **Conversion** of data from the **MS39 file** from *polar* to *Cartesian* coordinates.\n",
    "- The **matrices are now circular**, closely resembling the **natural shape of an eye**.\n",
    "- A **mask has been applied** to filter the data and **eliminate square borders**, reducing noise.\n",
    "- The exact number of **57,132 NaN** values corresponds to the **masked square border values**.\n",
    "- This transformation **significantly enhances** the data’s **anatomical fidelity**.\n",
    "- This version is **potentially final**, given the achieved **performance and accuracy**.\n",
    "- ⚡ **Possible improvement:** The `-1000` values were *not considered* in this process. Implementing them could further **refine precision**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130da2f8-ccd7-40a6-83de-832b0deac836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy.interpolate import griddata\n",
    "import cv2\n",
    "\"\"\"\n",
    "# ('name of segment', line where it starts, number of lines to read, low interval to take into account, high interval to take into account)\n",
    "# There must be a way of improving this\n",
    "\"\"\"\n",
    "segments = [\n",
    "    ('sagittal_anterior', 28, 27, -999, 10000),\n",
    "    ('tangential_anterior', 60, 27, -999, 10000),\n",
    "    ('gaussian_anterior', 92, 22, -999, 10000),\n",
    "    ('sagittal_posterior', 124, 26, -999, 10000),\n",
    "    ('tangential_posterior', 156, 26, -999, 10000),\n",
    "    ('gaussian_posterior', 188, 22, -999, 10000),\n",
    "    ('refra_frontal_power_anterior', 220, 27, -999, 10000),\n",
    "    ('refra_frontal_power_posterior', 252, 26, -999, 10000),\n",
    "    ('refra_equivalent_power', 284, 23, -999, 10000),\n",
    "    \"\"\"\n",
    "    Elevation maps in corneal topography require special processing. They use a reference surface, typically a Best Fitted Sphere (BFS),\n",
    "    which is calculated using a least weighted squares method. \n",
    "    The elevation data can then be analyzed using Zernike polynomials,\n",
    "    which are mathematical functions particularly useful for describing optical surfaces and wavefront aberrations.\n",
    "    However, some kind of unknown data transformation is done by the MS39 Machine, outputing all values between 0 and 3\n",
    "    Values are meaningful as they have p-values < 0.05 in statistical tests (correlations & mann-whitney keratoconus vs non-keratoconus).\n",
    "    \"\"\"\n",
    "    # ('elevation_anterior', 316, 27, -999, 10000),\n",
    "    # ('elevation_posterior', 348, 26, -999, 10000),\n",
    "    # ('elevation_stromal', 380, 25, -999, 10000),\n",
    "    ('corneal_thickness', 412, 26, -999, 10000),\n",
    "    ('stromal_thickness', 444, 25, -999, 10000),\n",
    "    ('epithelial_thickness', 476, 25, -999, 10000),\n",
    "    ('anterior_chamber_depth', 508, 26, -999, 10000)\n",
    "]\n",
    "\n",
    "def lire_segment(fichier, debut, n_lignes):\n",
    "    \"\"\"\n",
    "    Reads the segments. The key here is the skiprows function that allows us to bypass the particular\n",
    "    CSV format of the MS39 Machine. Indeed, the CSV file is quite particular with patient metadata (strings) everywhere.\n",
    "    Polars must be faster. However, at the date of creation of the code, there wasn't any equivalent of the skiprows parameter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(\n",
    "            fichier,\n",
    "            sep=';',\n",
    "            header=None,\n",
    "            skiprows=debut,\n",
    "            nrows=n_lignes,\n",
    "            usecols=range(256),\n",
    "            dtype=float\n",
    "        )\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Reading segment (lines {debut}:{debut+n_lignes}) : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def polar_to_cartesian(polar_df, target_size=512):\n",
    "    \"\"\"\n",
    "    Convert a polar matrix to cartesian, by admitting:\n",
    "      - Each column corresponds to a radial division (r = 0 -> 1).\n",
    "      - Each line corresponds to an angle (0° -> 360°).\n",
    "    \n",
    "    Function that transforms our polar matrix into a cartesian matrix (N*256 -> 512*512)\n",
    "\n",
    "    This function is the main core of the code.\n",
    "    We considered duplicating the last column for periodicity.\n",
    "    \"\"\"\n",
    "    # 1) Duplicate the first column for angle periodicity\n",
    "    df_periodic = pd.concat(\n",
    "        [polar_df, polar_df.iloc[:, [0]]], \n",
    "        axis=1,\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    rows, cols = df_periodic.shape  # Number of radial steps, number of angles + 1\n",
    "\n",
    "    # 2) Vectors for radius (r) and angle (theta)\n",
    "    r = np.linspace(0, 1, rows, endpoint=True)\n",
    "    theta = np.linspace(0, 2 * np.pi, cols, endpoint=True)\n",
    "\n",
    "    # 3) Polar grids (theta_grid, r_grid)\n",
    "    theta_grid, r_grid = np.meshgrid(theta, r, indexing='xy')\n",
    "\n",
    "    # 4) Conversion polar -> cartesian\n",
    "    x = r_grid * np.cos(theta_grid)\n",
    "    y = r_grid * np.sin(theta_grid)\n",
    "\n",
    "    # 5) Flatten valid values for interpolation\n",
    "    values = df_periodic.values\n",
    "    valid = ~np.isnan(values)\n",
    "    x_flat = x[valid]\n",
    "    y_flat = y[valid]\n",
    "    values_flat = values[valid]\n",
    "\n",
    "    # 6) Output cartesian grid\n",
    "    xi = np.linspace(-1, 1, target_size)\n",
    "    yi = np.linspace(-1, 1, target_size)\n",
    "    xi_grid, yi_grid = np.meshgrid(xi, yi)\n",
    "\n",
    "    # 7) Interpolation\n",
    "    cart_values = griddata(\n",
    "        (x_flat, y_flat),\n",
    "        values_flat,\n",
    "        (xi_grid, yi_grid),\n",
    "        method='linear'\n",
    "    )\n",
    "\n",
    "    # Replace NaN with mean value\n",
    "    mean_value = np.nanmean(values_flat)\n",
    "    cart_values[np.isnan(cart_values)] = mean_value\n",
    "\n",
    "    # 8) Smoothing with bilateral filter (OpenCV)\n",
    "    cart_values = cart_values.astype(np.float32, copy=False)\n",
    "    cart_values = cv2.bilateralFilter(cart_values, d=5, sigmaColor=40, sigmaSpace=50)\n",
    "\n",
    "    # 9) Mask for points outside the unit circle\n",
    "    R = np.sqrt(xi_grid**2 + yi_grid**2)\n",
    "    cart_values[R > 1.0] = np.nan\n",
    "\n",
    "    # Return as DataFrame\n",
    "    return pd.DataFrame(cart_values)\n",
    "\n",
    "def process_and_interpolate(fichier, segments):\n",
    "    \"\"\"\n",
    "    Create a Mask on the matrix to get only eye data (An eye is round which is incompatible with a block matrix)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for seg in segments:\n",
    "        name, start_row, num_rows, min_val, max_val = seg\n",
    "        start_time = time.time()\n",
    "        print(f\"[INFO] Processing segment: {name}\")\n",
    "        df = lire_segment(fichier, start_row, num_rows)\n",
    "        if df.empty:\n",
    "            print(f\"[WARNING] Segment {name} empty or impossible to read.\")\n",
    "            continue\n",
    "\n",
    "        # Filter out-of-bounds values\n",
    "        df = df.mask((df < min_val) | (df > max_val))\n",
    "        # -1000 => np.nan\n",
    "        df = df.replace(-1000, np.nan)\n",
    "        # Remove rows that are entirely NaN\n",
    "        df = df.dropna(axis=0, how='all')\n",
    "\n",
    "        nan_count = df.isna().sum().sum()\n",
    "        print(f\"[DEBUG] {name} - Dimensions before interp: {df.shape}, Number of NaN: {nan_count}\")\n",
    "\n",
    "        # Conversion polar->cartesian (512x512) + smoothing\n",
    "        cart_df = polar_to_cartesian(df, target_size=512)\n",
    "        nan_count_cart = cart_df.isna().sum().sum()\n",
    "        print(f\"[DEBUG] {name} - Dimensions after interp: {cart_df.shape}, Number of NaN: {nan_count_cart}\")\n",
    "\n",
    "        results[name] = cart_df\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"[INFO] Segment {name} processed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_to_hdf(results, output_file):\n",
    "    \"\"\"\n",
    "    Save all matrices to an HDF5 file.\n",
    "    Each segment is saved as a table (key=name).\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"[WARNING] No data to save.\")\n",
    "        return\n",
    "\n",
    "    # Open the HDF5 file in 'w' mode (overwrite) the first time,\n",
    "    # then 'a' (append) for subsequent segments.\n",
    "    first = True\n",
    "    for name, df in results.items():\n",
    "        mode = 'w' if first else 'a'\n",
    "        df.to_hdf(output_file, key=name, mode=mode)\n",
    "        first = False\n",
    "\n",
    "    print(f\"[INFO] Data saved in {output_file}\")\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Export all of the matrices into hdf5 format\n",
    "    .xlsx was too heavy and .npy introduced unknown bugs and variations in the final output.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            fichier = os.path.join(folder_path, filename)\n",
    "            # Output name .h5 instead of .xlsx\n",
    "            output_hdf_filename = os.path.splitext(filename)[0] + '.h5'\n",
    "            output_hdf_path = os.path.join(folder_path, output_hdf_filename)\n",
    "            print(f\"[INFO] Beginning processing of file {filename}\")\n",
    "            results = process_and_interpolate(fichier, segments)\n",
    "            save_to_hdf(results, output_hdf_path)\n",
    "            print(f\"[INFO] Finished processing {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r'Folder'\n",
    "    process_folder(folder_path)\n",
    "\n",
    "    \"\"\"\n",
    "    Onto Step 2 - Colormaps\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
